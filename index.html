<!DOCTYPE html>
<html>
<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-81D6829LG0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-81D6829LG0');
</script>


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "name": "DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description",
    "author": [
      {
        "@type": "Person",
        "name": "Adrienne Deganutti",
        "url": "https://www.linkedin.com/in/adrienne-deganutti-bb28031b6/"
      },
      {
        "@type": "Person",
        "name": "Simon Hadfield",
        "url": "https://www.surrey.ac.uk/people/simon-hadfield"
      },
      {
        "@type": "Person",
        "name": "Andrew Gilbert",
        "url": "https://andrewjohngilbert.github.io/"
      }
    ],
    "publisher": {
      "@type": "Organization",
      "name": "University of Surrey"
    },
    "datePublished": "2025-05-15",
    "url": "https://andrewjohngilbert.github.io/DANTE-AD/",
    "description": "DANTE-AD is a dual-vision Transformer model for long-term audio description, presented at CVPR 2025."
  }
  </script>

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Discover DANTE-AD, a state-of-the-art dual-vision Transformer model for long-term audio description. Learn more about our CVPR 2025 paper and results.">
  <meta property="og:title" content="DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description"/>
  <meta property="og:description" content="Discover DANTE-AD, a state-of-the-art dual-vision Transformer model for long-term audio description. Learn more about our CVPR 2025 paper and results."/>
  <meta property="og:url" content="https://andrewjohngilbert.github.io/DANTE-AD/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="assets/DANTE-ADTeaser.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description">
  <meta name="twitter:description" content="Discover DANTE-AD, a state-of-the-art dual-vision Transformer model for long-term audio description. Learn more about our CVPR 2025 paper and results.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="asserts/DANTE-ADTeaser.">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Machine Learning, Audio Description, Video Understanding,andrew Gilbert, simon hadfield, adrienne deganutti CVPR AICC 2025, AI for Content Creation Workshop, DANTE-AD, Dual-Vision Attention Network, Long-Term Audio Description, Video Description, Visual Storytelling, Transformer-based architecture, Sequential cross-attention, Contextual grounding, Fine-grained audio description generation, CMD-AD dataset, LLM-based evaluations">
  <meta name="author" content="Adrienne Deganutti, Simon Hadfield, Andrew Gilbert">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description</title>
 
  <link rel="icon" type="image/x-icon" href="static/images/favicon-32x32.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/adrienne-deganutti-bb28031b6/" target="_blank">Adrienne Deganutti</a>,</span>
                  <span class="author-block">
                    <a href="https://www.surrey.ac.uk/people/simon-hadfield" target="_blank">Simon Hadfield</a>,</span>
                  <span class="author-block">
                    <a href="https://andrewjohngilbert.github.io/" target="_blank">Andrew Gilbert</a>,</span>
            </div>

            <div class="is-size-5 publication-authors">
                    <span class="author-block"> University of Surrey <br> 
                      <a href="https://arxiv.org/abs/2503.24096" target="_blank">arXiv preprint arXiv:2503.24096,</a>  2025   <br>
                      <a href="https://cvpr.thecvf.com/Conferences/2025" target="_blank">The IEEE/CVF Conference on Computer Vision and Pattern Recognition 2025</a>
                      <a href="https://ai4cc.net/" target="_blank"> - Workshop on AI for Content Creation Workshop (AICC'25)</a></span>  

                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://andrewjohngilbert.github.io/DANTE-AD/assets/DANTE-ADPaper.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                     <!-- Arxiv PDF link -->
                     <span class="link-block">
                      <a href="https://arxiv.org/abs/2503.24096" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Arxiv</span>
                    </a>
                  </span>
 
                  <!-- Github link -->
                <span class="link-block">
                    <a href="https://github.com/Adrienne/DANTE-AD" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              

                
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser Image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/DANTE-ADWideTeaser.jpg" alt="DANTE-AD method overview for audio description">
      <h2 class="subtitle has-text-centered">
        Our DANTE-AD method extracts frame- and scene-level visual information fused via a sequential cross-attention module for both frame and scene context-aware AD over extended video sequences.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract: Advancing Audio Description with DANTE-AD</h2>
        <div class="content has-text-justified">
          <p>
            Audio Description is a narrated commentary designed to aid vision-impaired audiences in perceiving key visual elements in a video. While short-form video understanding has advanced rapidly, a solution for maintaining coherent long-term visual storytelling remains unresolved. Existing methods rely solely on frame-level embeddings, effectively describing object-based content but lacking contextual information across scenes. We introduce DANTE-AD, an enhanced video description model leveraging a dual-vision Transformer-based architecture to address this gap. DANTE-AD sequentially fuses both frame and scene level embeddings to improve long-term contextual understanding. We propose a novel, state-of-the-art method for sequential cross-attention to achieve contextual grounding for fine-grained audio description generation. Evaluated on a broad range of key scenes from well-known movie clips, DANTE-AD outperforms existing methods across traditional NLP metrics and LLM-based evaluations. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- System Figure-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/DANTE-AD-model-overview.jpg">
      <h2 class="subtitle has-text-centered">
        Overview of our audio description generation pipeline. The system features two primary branches: a frame-level visual branch (blue) and a scene-level visual branch (red). Ground-truth references are embedded and processed auto-regressively using a causal attention mask. Sequential fusion integrates the visual embeddings within the Dual-Vision Attention Network (purple). The fused representation is fed to our LLaMA language model and decoded into a natural language AD prediction.
      </h2>
    </div>
  </div>
</section>
<!-- End System Figure -->

<!-- Key innovation Figure-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/DANTE-AD-crossattention.jpg">
      <h2 class="subtitle has-text-centered">
        We propose a sequential fusion method within the Dual-Vision Attention Network to integrate frame- and scene-level embeddings. Ground-truth word embeddings are processed using a causal self-attention mask.
      </h2>
    </div>
  </div>
</section>
<!-- End Key innovation Figure -->

<!-- Table Results-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/DANTE-ADresultsTable.jpg">
      <h2 class="subtitle has-text-centered">
        Comparisons of AD performance on the CMD-AD dataset. LLM-AD-Eval is evaluated with LLaMA2-7B-chat (left) and GPT-3.5-turbo (right). We report the results for our method DANTE-AD using the sequential fusion of our visual embeddings.
      </h2>
    </div>
  </div>
</section>
<!-- End Table Results  -->

<!-- Results-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/Appendix-qualitative.jpg">
      <h2 class="subtitle has-text-centered">
        Qualitative results of our DANTE-AD method on CMD-AD-Eval. Our method uses sequential fusion cross-attention between frame- and scene-level visual embeddings.
      </h2>
    </div>
  </div>
</section>
<!-- End Results  -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{Deganutti:DANTE-AD:CVPRWS:2025,
        AUTHOR = Deganutti Adrienne, and  Hadfield, Simon and Gilbert, Andrew ",
        TITLE = "DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description",
        BOOKTITLE = "IEEE/CVF Conference on Computer Vision and Pattern Recognition - Workshop on AI for Content Creation Workshop (AICC'25)",
        YEAR = "2025",
        }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
